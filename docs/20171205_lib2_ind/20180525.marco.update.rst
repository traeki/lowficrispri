Near-term Agenda
++++++++++++++++

First Tier
----------

* Fix the test data splitting.
* Add evaluation/prediction of some sort.
* "delta" -- subtractive vs. geometric...
* filter guides with low gammas...

* sampling across genes or families...

Second Tier
-----------

* Add cross of mismatch location (?)
* Add GC content
* Add proximal bases

Third Tier
----------

* Experiment with Neural Nets, Elastic Net, SVM, ...
* Experiment with hyperparameters like optimizer
* Build stub version of absolute predictor
* Figure out inverted prediction (i.e. directly design guide for efficacy)

Jason/Carol/Marco meeting 20180511
==================================

* Train on lib2, check out lib3/4 outcomes
* Experiment with larger thresholds for dropping parents
* Continue with Tier 2 work

Meeting 20180517
================

* GC of parent as feature of delta
* Try
  * filtering on
  * or just using outright
  statistic of 5-fold groups

* Color datapoints with seed mismatches
* Get a handle on the TensorBoard and refactor

Current Meeting 20180517
========================

Third Tier
----------

* Experiment with Neural Nets, Elastic Net, SVM, ...
* Experiment with hyperparameters like optimizer
* Build stub version of absolute predictor
* Figure out inverted prediction (i.e. directly design guide for efficacy)

Other TODO
----------

* Add feature for first base of guide
* Graph parent guides colored by first base
* Also look at mismatches specifically in the first guide

* Compare to just using the position

* Mapping to "success" ... i.e. get 5 from 10, what are the odds, etc.

Once model looks better, revisit...:

* statistic aggregation/filtering
* filtering threshold
*** train on lib2 and evaluate against lib3/4 ***
* span relevance

Open Questions
--------------

* Why is there no 0,0 blob?
* What are the actual weights?

20180529 Plan
-------------

1. Create first-base feature and annotation

  * Plot first-base outcomes
  * Color first-base mismatches

2. Do comparison run with only position as feature.

3. Figure out how to find the actual weights in TF/TB

4. Think about how to assess "5 of 10" metric quantitatively.

5. Try other models:

   * Elastic Net & etc.
   * SVM
   * Neural Net variants

6. Revisit open questions in most promising model(s)

   * Are statistic aggregations better?
   * What's the best filtering threshold?
   * Which span gives best outcomes?
   * What do we learn about lib3/4 when training just on lib2?
   * Is there a blob at 0,0?
